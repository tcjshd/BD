{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "989b8619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 101.8543\n",
      "Epoch 10, Loss: 90.7653\n",
      "Epoch 20, Loss: 69.1502\n",
      "Epoch 30, Loss: 91.6093\n",
      "Epoch 40, Loss: 118.6902\n",
      "Epoch 50, Loss: 108.2234\n",
      "Epoch 60, Loss: 111.1921\n",
      "Epoch 70, Loss: 155.4534\n",
      "Epoch 80, Loss: 185.8189\n",
      "Epoch 90, Loss: 113.8598\n",
      "Sampled text: hhMhPr hhh \n"
     ]
    }
   ],
   "source": [
    "import numpy as np  # We use NumPy to do matrix math easily\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: Prepare Dataset\n",
    "# -----------------------------\n",
    "\n",
    "data = \"I am Parth Gala My friend is Rachit\"  # This is the text we want our RNN to learn\n",
    "chars = list(set(data))  # Get all unique characters, like ['h', 'e', 'l', 'o']\n",
    "vocab_size = len(chars)  # Number of unique characters\n",
    "\n",
    "# Map characters to numbers (like 'h' → 0, 'e' → 1, etc.)\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for ch, i in char_to_idx.items()}\n",
    "\n",
    "# Convert a character to one-hot vector\n",
    "def one_hot_encode(char, char_to_idx):\n",
    "    vec = np.zeros((len(char_to_idx), 1))  # Create a column of zeros\n",
    "    vec[char_to_idx[char]] = 1  # Set the position of the character to 1\n",
    "    return vec\n",
    "\n",
    "# Softmax function to turn scores into probabilities\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))  # Stability trick\n",
    "    return e_x / np.sum(e_x, axis=0)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Define the RNN Class\n",
    "# -----------------------------\n",
    "class RNN:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size  # How many neurons in the hidden layer\n",
    "\n",
    "        # Weight matrices and biases (randomly initialized small numbers)\n",
    "        self.U = np.random.randn(hidden_size, input_size) * 0.01  # input → hidden\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden → hidden (previous step)\n",
    "        self.V = np.random.randn(output_size, hidden_size) * 0.01  # hidden → output\n",
    "\n",
    "        self.b = np.zeros((hidden_size, 1))  # bias for hidden layer\n",
    "        self.c = np.zeros((output_size, 1))  # bias for output layer\n",
    "\n",
    "    # Forward pass: go through all characters in the input\n",
    "    def forward(self, inputs, h_prev):\n",
    "        xs, hs, os, ps = {}, {}, {}, {}  # Store data at each time step\n",
    "        hs[-1] = np.copy(h_prev)  # Set initial hidden state\n",
    "\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = inputs[t]  # Store input\n",
    "            hs[t] = np.tanh(np.dot(self.U, xs[t]) + np.dot(self.W, hs[t - 1]) + self.b)  # Hidden state\n",
    "            os[t] = np.dot(self.V, hs[t]) + self.c  # Raw scores (logits)\n",
    "            ps[t] = softmax(os[t])  # Probabilities\n",
    "        return xs, hs, os, ps\n",
    "\n",
    "    # Backpropagation Through Time: calculate loss and gradients\n",
    "    def loss_and_gradients(self, inputs, targets, h_prev):\n",
    "        xs, hs, os, ps = self.forward(inputs, h_prev)  # Forward pass first\n",
    "        loss = 0  # Total loss\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)  # Gradients\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dh_next = np.zeros_like(hs[0])  # For passing gradient backward\n",
    "\n",
    "        # Calculate loss\n",
    "        for t in range(len(inputs)):\n",
    "            loss += -np.log(ps[t][targets[t], 0])  # Cross-entropy loss\n",
    "\n",
    "        # Backward pass (back through time)\n",
    "        for t in reversed(range(len(inputs))):\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1  # Gradient of softmax loss\n",
    "\n",
    "            dV += np.dot(dy, hs[t].T)  # Gradient wrt V\n",
    "            dc += dy  # Gradient wrt output bias\n",
    "\n",
    "            dh = np.dot(self.V.T, dy) + dh_next  # Gradient of hidden layer\n",
    "            dh_raw = (1 - hs[t] ** 2) * dh  # tanh derivative\n",
    "\n",
    "            db += dh_raw\n",
    "            dU += np.dot(dh_raw, xs[t].T)\n",
    "            dW += np.dot(dh_raw, hs[t - 1].T)\n",
    "\n",
    "            dh_next = np.dot(self.W.T, dh_raw)  # Pass gradient to previous time\n",
    "\n",
    "        return loss, dU, dW, dV, db, dc, hs[len(inputs) - 1]  # Return everything we need\n",
    "\n",
    "    # Update weights using simple gradient descent\n",
    "    def update(self, grads, learning_rate=0.1):\n",
    "        dU, dW, dV, db, dc = grads\n",
    "        for param, dparam in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                 [dU, dW, dV, db, dc]):\n",
    "            param -= learning_rate * dparam  # Move weights in opposite direction of gradient\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Train the RNN\n",
    "# -----------------------------\n",
    "rnn = RNN(input_size=vocab_size, hidden_size=10, output_size=vocab_size)  # Create RNN\n",
    "h_prev = np.zeros((10, 1))  # Start with all hidden states as 0\n",
    "\n",
    "for epoch in range(100):  # Train for 100 loops\n",
    "    # Prepare input and target\n",
    "    inputs = [one_hot_encode(ch, char_to_idx) for ch in data[:-1]]  # \"hell\"\n",
    "    targets = [char_to_idx[ch] for ch in data[1:]]  # \"ello\"\n",
    "\n",
    "    # Compute loss and gradients\n",
    "    loss, dU, dW, dV, db, dc, h_prev = rnn.loss_and_gradients(inputs, targets, h_prev)\n",
    "\n",
    "    # Update weights using gradients\n",
    "    rnn.update((dU, dW, dV, db, dc), learning_rate=0.1)\n",
    "\n",
    "    # Print progress every 10 steps\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Generate Text (Sampling)\n",
    "# -----------------------------\n",
    "def sample(rnn, seed_char, char_to_idx, idx_to_char, n=5):\n",
    "    x = one_hot_encode(seed_char, char_to_idx)  # Convert starting char to one-hot\n",
    "    h = np.zeros((rnn.hidden_size, 1))  # Start with blank hidden state\n",
    "    output = seed_char  # Start output with the seed char\n",
    "\n",
    "    for _ in range(n):\n",
    "        _, hs, _, ps = rnn.forward([x], h)  # Forward pass for one character\n",
    "        h = hs[0]  # Get updated hidden state\n",
    "        idx = np.random.choice(range(len(char_to_idx)), p=ps[0].ravel())  # Pick next char by probability\n",
    "        x = one_hot_encode(idx_to_char[idx], char_to_idx)  # New input\n",
    "        output += idx_to_char[idx]  # Add predicted char to output\n",
    "    return output\n",
    "\n",
    "# Try generating text!\n",
    "print(\"Sampled text:\", sample(rnn, seed_char=\"h\", char_to_idx=char_to_idx, idx_to_char=idx_to_char, n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c2f5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
