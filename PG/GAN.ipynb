{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46bb3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aec15cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [100/10000], d_loss: 1.4027, g_loss: 0.6805\n",
      "Epoch [200/10000], d_loss: 1.3835, g_loss: 0.6989\n",
      "Epoch [300/10000], d_loss: 1.3840, g_loss: 0.6952\n",
      "Epoch [400/10000], d_loss: 1.3995, g_loss: 0.6817\n",
      "Epoch [500/10000], d_loss: 1.3941, g_loss: 0.7044\n",
      "Epoch [600/10000], d_loss: 1.3815, g_loss: 0.6988\n",
      "Epoch [700/10000], d_loss: 1.4052, g_loss: 0.6785\n",
      "Epoch [800/10000], d_loss: 1.3910, g_loss: 0.7015\n",
      "Epoch [900/10000], d_loss: 1.4050, g_loss: 0.6783\n",
      "Epoch [1000/10000], d_loss: 1.3845, g_loss: 0.7001\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [1100/10000], d_loss: 1.3849, g_loss: 0.6960\n",
      "Epoch [1200/10000], d_loss: 1.3864, g_loss: 0.6943\n",
      "Epoch [1300/10000], d_loss: 1.3816, g_loss: 0.6995\n",
      "Epoch [1400/10000], d_loss: 1.3743, g_loss: 0.7073\n",
      "Epoch [1500/10000], d_loss: 1.3872, g_loss: 0.6940\n",
      "Epoch [1600/10000], d_loss: 1.3543, g_loss: 0.7410\n",
      "Epoch [1700/10000], d_loss: 1.3837, g_loss: 0.6979\n",
      "Epoch [1800/10000], d_loss: 1.4023, g_loss: 0.6918\n",
      "Epoch [1900/10000], d_loss: 1.4462, g_loss: 0.6676\n",
      "Epoch [2000/10000], d_loss: 1.3778, g_loss: 0.7088\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [2100/10000], d_loss: 1.3772, g_loss: 0.7131\n",
      "Epoch [2200/10000], d_loss: 1.3854, g_loss: 0.7239\n",
      "Epoch [2300/10000], d_loss: 1.4052, g_loss: 0.6789\n",
      "Epoch [2400/10000], d_loss: 1.3743, g_loss: 0.7111\n",
      "Epoch [2500/10000], d_loss: 1.3593, g_loss: 0.7270\n",
      "Epoch [2600/10000], d_loss: 1.4122, g_loss: 0.7067\n",
      "Epoch [2700/10000], d_loss: 1.4035, g_loss: 0.6809\n",
      "Epoch [2800/10000], d_loss: 1.3942, g_loss: 0.6861\n",
      "Epoch [2900/10000], d_loss: 1.3753, g_loss: 0.7098\n",
      "Epoch [3000/10000], d_loss: 1.3908, g_loss: 0.6938\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [3100/10000], d_loss: 1.3868, g_loss: 0.6928\n",
      "Epoch [3200/10000], d_loss: 1.4018, g_loss: 0.6850\n",
      "Epoch [3300/10000], d_loss: 1.3831, g_loss: 0.7051\n",
      "Epoch [3400/10000], d_loss: 1.3673, g_loss: 0.7195\n",
      "Epoch [3500/10000], d_loss: 1.4013, g_loss: 0.6845\n",
      "Epoch [3600/10000], d_loss: 1.3915, g_loss: 0.7059\n",
      "Epoch [3700/10000], d_loss: 1.3743, g_loss: 0.7116\n",
      "Epoch [3800/10000], d_loss: 1.4499, g_loss: 0.6723\n",
      "Epoch [3900/10000], d_loss: 1.4263, g_loss: 0.6799\n",
      "Epoch [4000/10000], d_loss: 1.4338, g_loss: 0.7107\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [4100/10000], d_loss: 1.3916, g_loss: 0.7020\n",
      "Epoch [4200/10000], d_loss: 1.3797, g_loss: 0.6999\n",
      "Epoch [4300/10000], d_loss: 1.3764, g_loss: 0.7147\n",
      "Epoch [4400/10000], d_loss: 1.3910, g_loss: 0.7022\n",
      "Epoch [4500/10000], d_loss: 1.3721, g_loss: 0.7315\n",
      "Epoch [4600/10000], d_loss: 1.4151, g_loss: 0.6731\n",
      "Epoch [4700/10000], d_loss: 1.4424, g_loss: 0.6637\n",
      "Epoch [4800/10000], d_loss: 1.3685, g_loss: 0.7156\n",
      "Epoch [4900/10000], d_loss: 1.4393, g_loss: 0.6819\n",
      "Epoch [5000/10000], d_loss: 1.3635, g_loss: 0.7199\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [5100/10000], d_loss: 1.3907, g_loss: 0.6914\n",
      "Epoch [5200/10000], d_loss: 1.4150, g_loss: 0.6724\n",
      "Epoch [5300/10000], d_loss: 1.3617, g_loss: 0.7213\n",
      "Epoch [5400/10000], d_loss: 1.3877, g_loss: 0.7047\n",
      "Epoch [5500/10000], d_loss: 1.3841, g_loss: 0.6954\n",
      "Epoch [5600/10000], d_loss: 1.4025, g_loss: 0.6857\n",
      "Epoch [5700/10000], d_loss: 1.3934, g_loss: 0.6903\n",
      "Epoch [5800/10000], d_loss: 1.3762, g_loss: 0.7398\n",
      "Epoch [5900/10000], d_loss: 1.4173, g_loss: 0.7271\n",
      "Epoch [6000/10000], d_loss: 1.3820, g_loss: 0.6996\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [6100/10000], d_loss: 1.3818, g_loss: 0.7025\n",
      "Epoch [6200/10000], d_loss: 1.3875, g_loss: 0.6918\n",
      "Epoch [6300/10000], d_loss: 1.3925, g_loss: 0.6948\n",
      "Epoch [6400/10000], d_loss: 1.3997, g_loss: 0.6811\n",
      "Epoch [6500/10000], d_loss: 1.4199, g_loss: 0.6726\n",
      "Epoch [6600/10000], d_loss: 1.3891, g_loss: 0.7012\n",
      "Epoch [6700/10000], d_loss: 1.3940, g_loss: 0.6886\n",
      "Epoch [6800/10000], d_loss: 1.3926, g_loss: 0.6861\n",
      "Epoch [6900/10000], d_loss: 1.4028, g_loss: 0.6854\n",
      "Epoch [7000/10000], d_loss: 1.3875, g_loss: 0.6990\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [7100/10000], d_loss: 1.3940, g_loss: 0.6900\n",
      "Epoch [7200/10000], d_loss: 1.3758, g_loss: 0.7126\n",
      "Epoch [7300/10000], d_loss: 1.4104, g_loss: 0.6850\n",
      "Epoch [7400/10000], d_loss: 1.3832, g_loss: 0.7021\n",
      "Epoch [7500/10000], d_loss: 1.3984, g_loss: 0.6816\n",
      "Epoch [7600/10000], d_loss: 1.3844, g_loss: 0.6960\n",
      "Epoch [7700/10000], d_loss: 1.3853, g_loss: 0.7052\n",
      "Epoch [7800/10000], d_loss: 1.3875, g_loss: 0.6935\n",
      "Epoch [7900/10000], d_loss: 1.4228, g_loss: 0.6663\n",
      "Epoch [8000/10000], d_loss: 1.4083, g_loss: 0.6841\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [8100/10000], d_loss: 1.3878, g_loss: 0.6918\n",
      "Epoch [8200/10000], d_loss: 1.3232, g_loss: 0.7769\n",
      "Epoch [8300/10000], d_loss: 1.4069, g_loss: 0.6761\n",
      "Epoch [8400/10000], d_loss: 1.3964, g_loss: 0.6864\n",
      "Epoch [8500/10000], d_loss: 1.3727, g_loss: 0.7212\n",
      "Epoch [8600/10000], d_loss: 1.3888, g_loss: 0.6936\n",
      "Epoch [8700/10000], d_loss: 1.3898, g_loss: 0.7132\n",
      "Epoch [8800/10000], d_loss: 1.3880, g_loss: 0.6922\n",
      "Epoch [8900/10000], d_loss: 1.3667, g_loss: 0.7283\n",
      "Epoch [9000/10000], d_loss: 1.3799, g_loss: 0.7031\n",
      "z shape:  (32, 100)\n",
      "weights_g shape:  (100, 2)\n",
      "real_batch shape:  (32, 2)\n",
      "Epoch [9100/10000], d_loss: 1.3963, g_loss: 0.6976\n",
      "Epoch [9200/10000], d_loss: 1.4042, g_loss: 0.6890\n",
      "Epoch [9300/10000], d_loss: 1.3868, g_loss: 0.6986\n",
      "Epoch [9400/10000], d_loss: 1.3719, g_loss: 0.7211\n",
      "Epoch [9500/10000], d_loss: 1.3890, g_loss: 0.6997\n",
      "Epoch [9600/10000], d_loss: 1.4154, g_loss: 0.6721\n",
      "Epoch [9700/10000], d_loss: 1.3826, g_loss: 0.6993\n",
      "Epoch [9800/10000], d_loss: 1.3848, g_loss: 0.6947\n",
      "Epoch [9900/10000], d_loss: 1.3783, g_loss: 0.7149\n",
      "Epoch [10000/10000], d_loss: 1.4038, g_loss: 0.7044\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generator network: simple linear transformation\n",
    "def generator(z, weights_g):\n",
    "    return np.dot(z, weights_g)\n",
    "\n",
    "# Discriminator network: simple linear transformation\n",
    "def discriminator(x, weights_d):\n",
    "    return np.dot(x, weights_d)\n",
    "\n",
    "# Sigmoid activation function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 10000\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "latent_dim = 100\n",
    "data_dim = 2\n",
    "num_examples = 1000\n",
    "eps = 1e-8  # For numerical stability in log\n",
    "\n",
    "# Generate sample real data: 2D points along y = -x\n",
    "real_data = np.random.normal(loc=0, scale=1.0, size=(num_examples, data_dim))\n",
    "real_data[:, 1] = -real_data[:, 0]\n",
    "\n",
    "# Initialize weights\n",
    "weights_g = np.random.normal(size=(latent_dim, data_dim))   # (100, 2)\n",
    "weights_d = np.random.normal(size=(data_dim, 1))            # (2, 1)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Sample real data\n",
    "    idx = np.random.randint(0, num_examples, batch_size)\n",
    "    real_batch = real_data[idx]                              # (32, 2)\n",
    "\n",
    "    # Generate fake data\n",
    "    z = np.random.normal(size=(batch_size, latent_dim))      # (32, 100)\n",
    "    fake_data = generator(z, weights_g)                      # (32, 2)\n",
    "    if(epoch % 1000 == 0):\n",
    "        print(\"z shape: \", z.shape)\n",
    "        print(\"weights_g shape: \", weights_g.shape)\n",
    "        print(\"real_batch shape: \", real_batch.shape)\n",
    "\n",
    "    # Discriminator outputs\n",
    "    d_real = discriminator(real_batch, weights_d)            # (32, 1)\n",
    "    d_fake = discriminator(fake_data, weights_d)             # (32, 1)\n",
    "\n",
    "    # Apply sigmoid\n",
    "    sig_d_real = sigmoid(d_real)\n",
    "    sig_d_fake = sigmoid(d_fake)\n",
    "\n",
    "    # Discriminator loss (with stability clipping)\n",
    "    d_loss_real = -np.mean(np.log(np.clip(sig_d_real, eps, 1 - eps)))\n",
    "    d_loss_fake = -np.mean(np.log(np.clip(1 - sig_d_fake, eps, 1 - eps)))\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "    # Gradients for discriminator\n",
    "    grad_real = np.dot(real_batch.T, (sig_d_real - 1)) / batch_size    # (2, 32) @ (32, 1) → (2, 1)\n",
    "    grad_fake = np.dot(fake_data.T, sig_d_fake) / batch_size\n",
    "    d_weights_d = grad_real + grad_fake                                # (2, 1)\n",
    "    weights_d -= learning_rate * d_weights_d                           # Update\n",
    "\n",
    "    # Generator loss\n",
    "    g_loss = -np.mean(np.log(np.clip(sig_d_fake, eps, 1 - eps)))\n",
    "\n",
    "    # Gradients for generator (discriminator is linear so backprop is simple)\n",
    "    g_grad = np.dot(z.T, sig_d_fake) / batch_size                      # (100, 32) @ (32, 1) → (100, 1)\n",
    "    weights_g -= learning_rate * g_grad                                # Update\n",
    "\n",
    "    # Print progress\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], d_loss: {d_loss:.4f}, g_loss: {g_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "470300b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Data: [[1.19985591 1.67641229]]\n"
     ]
    }
   ],
   "source": [
    "# After training, you can sample data from the generator\n",
    "sampled_data = generator(np.random.normal(size=(1, latent_dim)), weights_g)\n",
    "print(\"Sampled Data:\", sampled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597cc35a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
