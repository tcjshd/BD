{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPd0FeQmoxd0bjHdd8HLaT/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","\n","# Generator network: simple linear transformation\n","def generator(z, weights_g):\n","    return np.dot(z, weights_g)\n","\n","# Discriminator network: simple linear transformation\n","def discriminator(x, weights_d):\n","    return np.dot(x, weights_d)\n","\n","# Sigmoid activation function\n","def sigmoid(x):\n","    return 1 / (1 + np.exp(-x))\n","\n","# Hyperparameters\n","epochs = 10000\n","learning_rate = 0.01\n","batch_size = 32\n","latent_dim = 100\n","data_dim = 2\n","num_examples = 1000\n","eps = 1e-8  # For numerical stability in log\n","\n","# Generate sample real data: 2D points along y = -x\n","real_data = np.random.normal(loc=0, scale=1.0, size=(num_examples, data_dim))\n","real_data[:, 1] = -real_data[:, 0]\n","\n","# Initialize weights\n","weights_g = np.random.normal(size=(latent_dim, data_dim))   # (100, 2)\n","weights_d = np.random.normal(size=(data_dim, 1))            # (2, 1)\n","\n","for epoch in range(epochs):\n","\n","    # Sample real data\n","    idx = np.random.randint(0, num_examples, batch_size)\n","    real_batch = real_data[idx]                              # (32, 2)\n","\n","    # Generate fake data\n","    z = np.random.normal(size=(batch_size, latent_dim))      # (32, 100)\n","    fake_data = generator(z, weights_g)                      # (32, 2)\n","\n","    # Discriminator outputs\n","    d_real = discriminator(real_batch, weights_d)            # (32, 1)\n","    d_fake = discriminator(fake_data, weights_d)             # (32, 1)\n","\n","    # Apply sigmoid\n","    sig_d_real = sigmoid(d_real)\n","    sig_d_fake = sigmoid(d_fake)\n","\n","    # Discriminator loss (with stability clipping)\n","    d_loss_real = -np.mean(np.log(np.clip(sig_d_real, eps, 1 - eps)))\n","    d_loss_fake = -np.mean(np.log(np.clip(1 - sig_d_fake, eps, 1 - eps)))\n","    d_loss = d_loss_real + d_loss_fake\n","\n","    # Gradients for discriminator\n","    grad_real = np.dot(real_batch.T, (sig_d_real - 1)) / batch_size    # (2, 32) @ (32, 1) → (2, 1)\n","    grad_fake = np.dot(fake_data.T, sig_d_fake) / batch_size\n","    d_weights_d = grad_real + grad_fake                                # (2, 1)\n","    weights_d -= learning_rate * d_weights_d                           # Update\n","\n","    # Generator loss\n","    g_loss = -np.mean(np.log(np.clip(sig_d_fake, eps, 1 - eps)))\n","\n","    # Gradients for generator (discriminator is linear so backprop is simple)\n","    g_grad = np.dot(z.T, sig_d_fake) / batch_size                      # (100, 32) @ (32, 1) → (100, 1)\n","    weights_g -= learning_rate * g_grad                                # Update\n","\n","    # Print progress\n","    if (epoch + 1) % 100 == 0:\n","        print(f\"Epoch [{epoch+1}/{epochs}], d_loss: {d_loss:.4f}, g_loss: {g_loss:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hkgGinjncZAX","executionInfo":{"status":"ok","timestamp":1744353429572,"user_tz":-330,"elapsed":2396,"user":{"displayName":"Dragon Rider","userId":"18245359096310103758"}},"outputId":"47772550-65a0-4353-9c9c-c396258b862d"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [100/10000], d_loss: 1.3911, g_loss: 0.7225\n","Epoch [200/10000], d_loss: 1.3891, g_loss: 0.6913\n","Epoch [300/10000], d_loss: 1.3790, g_loss: 0.7209\n","Epoch [400/10000], d_loss: 1.4035, g_loss: 0.6802\n","Epoch [500/10000], d_loss: 1.3999, g_loss: 0.6930\n","Epoch [600/10000], d_loss: 1.3926, g_loss: 0.7044\n","Epoch [700/10000], d_loss: 1.3687, g_loss: 0.7472\n","Epoch [800/10000], d_loss: 1.3838, g_loss: 0.7067\n","Epoch [900/10000], d_loss: 1.3875, g_loss: 0.7292\n","Epoch [1000/10000], d_loss: 1.4267, g_loss: 0.6705\n","Epoch [1100/10000], d_loss: 1.3874, g_loss: 0.6980\n","Epoch [1200/10000], d_loss: 1.3970, g_loss: 0.6906\n","Epoch [1300/10000], d_loss: 1.3966, g_loss: 0.6990\n","Epoch [1400/10000], d_loss: 1.4011, g_loss: 0.6871\n","Epoch [1500/10000], d_loss: 1.4023, g_loss: 0.7054\n","Epoch [1600/10000], d_loss: 1.3784, g_loss: 0.7311\n","Epoch [1700/10000], d_loss: 1.3865, g_loss: 0.6977\n","Epoch [1800/10000], d_loss: 1.4030, g_loss: 0.6885\n","Epoch [1900/10000], d_loss: 1.4324, g_loss: 0.6581\n","Epoch [2000/10000], d_loss: 1.3771, g_loss: 0.7033\n","Epoch [2100/10000], d_loss: 1.3904, g_loss: 0.6928\n","Epoch [2200/10000], d_loss: 1.3803, g_loss: 0.7016\n","Epoch [2300/10000], d_loss: 1.4029, g_loss: 0.6880\n","Epoch [2400/10000], d_loss: 1.4024, g_loss: 0.6870\n","Epoch [2500/10000], d_loss: 1.4072, g_loss: 0.6812\n","Epoch [2600/10000], d_loss: 1.4133, g_loss: 0.6808\n","Epoch [2700/10000], d_loss: 1.3595, g_loss: 0.7251\n","Epoch [2800/10000], d_loss: 1.3864, g_loss: 0.6950\n","Epoch [2900/10000], d_loss: 1.3958, g_loss: 0.6879\n","Epoch [3000/10000], d_loss: 1.3833, g_loss: 0.7011\n","Epoch [3100/10000], d_loss: 1.3819, g_loss: 0.6990\n","Epoch [3200/10000], d_loss: 1.3897, g_loss: 0.6917\n","Epoch [3300/10000], d_loss: 1.3962, g_loss: 0.7049\n","Epoch [3400/10000], d_loss: 1.3293, g_loss: 0.7674\n","Epoch [3500/10000], d_loss: 1.3711, g_loss: 0.7132\n","Epoch [3600/10000], d_loss: 1.4016, g_loss: 0.6898\n","Epoch [3700/10000], d_loss: 1.3802, g_loss: 0.7032\n","Epoch [3800/10000], d_loss: 1.3898, g_loss: 0.6923\n","Epoch [3900/10000], d_loss: 1.4049, g_loss: 0.6863\n","Epoch [4000/10000], d_loss: 1.3872, g_loss: 0.6937\n","Epoch [4100/10000], d_loss: 1.3890, g_loss: 0.6907\n","Epoch [4200/10000], d_loss: 1.3948, g_loss: 0.7035\n","Epoch [4300/10000], d_loss: 1.3958, g_loss: 0.6993\n","Epoch [4400/10000], d_loss: 1.3858, g_loss: 0.6940\n","Epoch [4500/10000], d_loss: 1.3856, g_loss: 0.6943\n","Epoch [4600/10000], d_loss: 1.3766, g_loss: 0.7024\n","Epoch [4700/10000], d_loss: 1.3878, g_loss: 0.6919\n","Epoch [4800/10000], d_loss: 1.3727, g_loss: 0.7122\n","Epoch [4900/10000], d_loss: 1.3994, g_loss: 0.6866\n","Epoch [5000/10000], d_loss: 1.3782, g_loss: 0.7028\n","Epoch [5100/10000], d_loss: 1.4118, g_loss: 0.6786\n","Epoch [5200/10000], d_loss: 1.3173, g_loss: 0.8089\n","Epoch [5300/10000], d_loss: 1.3789, g_loss: 0.7373\n","Epoch [5400/10000], d_loss: 1.4225, g_loss: 0.6682\n","Epoch [5500/10000], d_loss: 1.3992, g_loss: 0.6886\n","Epoch [5600/10000], d_loss: 1.4310, g_loss: 0.6747\n","Epoch [5700/10000], d_loss: 1.3805, g_loss: 0.7016\n","Epoch [5800/10000], d_loss: 1.3732, g_loss: 0.7193\n","Epoch [5900/10000], d_loss: 1.3918, g_loss: 0.6904\n","Epoch [6000/10000], d_loss: 1.3914, g_loss: 0.6984\n","Epoch [6100/10000], d_loss: 1.3819, g_loss: 0.7019\n","Epoch [6200/10000], d_loss: 1.3874, g_loss: 0.6925\n","Epoch [6300/10000], d_loss: 1.3806, g_loss: 0.7004\n","Epoch [6400/10000], d_loss: 1.3689, g_loss: 0.7289\n","Epoch [6500/10000], d_loss: 1.4078, g_loss: 0.6770\n","Epoch [6600/10000], d_loss: 1.3774, g_loss: 0.7115\n","Epoch [6700/10000], d_loss: 1.4041, g_loss: 0.6782\n","Epoch [6800/10000], d_loss: 1.3503, g_loss: 0.7519\n","Epoch [6900/10000], d_loss: 1.3719, g_loss: 0.7158\n","Epoch [7000/10000], d_loss: 1.3932, g_loss: 0.6865\n","Epoch [7100/10000], d_loss: 1.3694, g_loss: 0.7395\n","Epoch [7200/10000], d_loss: 1.3926, g_loss: 0.6943\n","Epoch [7300/10000], d_loss: 1.3875, g_loss: 0.6961\n","Epoch [7400/10000], d_loss: 1.3936, g_loss: 0.6870\n","Epoch [7500/10000], d_loss: 1.4470, g_loss: 0.6641\n","Epoch [7600/10000], d_loss: 1.3762, g_loss: 0.7061\n","Epoch [7700/10000], d_loss: 1.4170, g_loss: 0.6833\n","Epoch [7800/10000], d_loss: 1.4516, g_loss: 0.6598\n","Epoch [7900/10000], d_loss: 1.3991, g_loss: 0.6838\n","Epoch [8000/10000], d_loss: 1.3952, g_loss: 0.6865\n","Epoch [8100/10000], d_loss: 1.3925, g_loss: 0.6890\n","Epoch [8200/10000], d_loss: 1.3795, g_loss: 0.7192\n","Epoch [8300/10000], d_loss: 1.3838, g_loss: 0.6995\n","Epoch [8400/10000], d_loss: 1.3637, g_loss: 0.7393\n","Epoch [8500/10000], d_loss: 1.4210, g_loss: 0.6744\n","Epoch [8600/10000], d_loss: 1.4986, g_loss: 0.6415\n","Epoch [8700/10000], d_loss: 1.3895, g_loss: 0.6902\n","Epoch [8800/10000], d_loss: 1.3949, g_loss: 0.6938\n","Epoch [8900/10000], d_loss: 1.4044, g_loss: 0.6825\n","Epoch [9000/10000], d_loss: 1.4039, g_loss: 0.6842\n","Epoch [9100/10000], d_loss: 1.3663, g_loss: 0.7156\n","Epoch [9200/10000], d_loss: 1.4069, g_loss: 0.6823\n","Epoch [9300/10000], d_loss: 1.4005, g_loss: 0.6913\n","Epoch [9400/10000], d_loss: 1.4024, g_loss: 0.7070\n","Epoch [9500/10000], d_loss: 1.3718, g_loss: 0.7202\n","Epoch [9600/10000], d_loss: 1.3656, g_loss: 0.7405\n","Epoch [9700/10000], d_loss: 1.3954, g_loss: 0.6878\n","Epoch [9800/10000], d_loss: 1.3931, g_loss: 0.6874\n","Epoch [9900/10000], d_loss: 1.3937, g_loss: 0.6989\n","Epoch [10000/10000], d_loss: 1.3785, g_loss: 0.7087\n"]}]},{"cell_type":"code","source":["# After training, you can sample data from the generator\n","sampled_data = generator(np.random.normal(size=(1, latent_dim)), weights_g)\n","print(\"Sampled Data:\", sampled_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqTY0qssY8gA","executionInfo":{"status":"ok","timestamp":1744353448992,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dragon Rider","userId":"18245359096310103758"}},"outputId":"c2450727-dc62-4a9b-89d2-7f601bcc2efc"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Sampled Data: [[ 6.87059281 13.25175218]]\n"]}]}]}